Tuesday, July 22, 2014
-----------------------------

-----------------------------
Building UV-CDAT
-----------------------------

This is the path!
/usr/bin:/bin:/usr/sbin:/sbin:/Applications/CMake_2.8-12.app/Contents/bin

symlinked /usr/local/bin/gfortran to /usr/bin

[https://github.com/UV-CDAT/uvcdat/wiki/Building-UVCDAT]
git clone uvcdat (and uvcdat-devel, see instructions on website)
mkdir uvcdat-build
cd uvcdat-build
ccmake ../uvcdat
set http:// for git protocol (if inside the lab)
c, c, c, g
make -j8

source ./install/bin/setup_runtime.sh

-------------------------------------------------------------------------------

#dumps all output to console
./install/bin/uvcdat --output=""

-------------------------------------------------------------------------------

-----------------------------
ViSUS Installation
-----------------------------

Build visuspy:
export PATH=/Users/cam/code/uvcdat-build/install/Library/Frameworks/Python.framework/Versions/2.7/bin:$PATH
ccmake -DVISUS_SWIG=YES ../..
(NOTE: Still had to manually change PYTHON_INCLUDE_DIR and PYTHON_LIBRARY to point to the correct places in the uvcdat installation)


#Install visuspy to <uvcdat_install>/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages
<visus>/visuscpp/swig/installpy.sh /Users/cam/code/uvcdat-build/install/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages

mkpy.sh
testpy.sh
installpy.sh

***
UPDATE: scripts will now automatically find python location, so do this:
 (*no they won't, must manually set PYTHON_INCLUDE_DIR and PYTHON_LIBRARY)
***

source ~/code/uvcdat-build/install/bin/setup_runtime.sh

ccmake -DPYTHON_INCLUDE_DIR=/Users/cam/code/uvcdat-build/install/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -DPYTHON_LIBRARY=/Users/cam/code/uvcdat-build/install/Library/Frameworks/Python.framework/Versions/2.7/lib/libpython2.7.dylib -DVISUS_SWIG=YES ../..
make -j12
../../visuscpp/swig/installpy.sh


-------------------------------------------------------------------------------

Sunday, December 07, 2014

[ ] cron script to monitor directory and restart server if a new idx file appears
[x] script to create initial tables in idx.db
[ ] xml_to_idx script to create idx and populate idx.db tables
[ ] converter app (can be part of server, facilitates Timo's way) to convert a var at a timestep upon request
[ ] server modified to check idx.db to see if a variable has already been converted (not final solution)

[ ] for conversion, may not need to use db. Also, cdms2 basically eats that xml file and takes care of everything.

-------------------------------------------------------------------------------

Monday, April 20, 2015

We have not really created a suitable mapping from cdat datasets to idx volumes. Most datasets contain fiends from many different domains, which require separate idx volumes. It is necessary to adopt a standard naming scheme that is clear, related to the original dataset(s), and as simple as possible. We also need an interface on the reader side to list the variables of a dataset since seemingly related variables may be in different domains ("plev lat lon" vs "lev lat lon") even though the domains are the same size. We can choose to identify datasets only by their shape, but then we are throwing away information (plev vs lev) that may be critical for rendering and analysis.

-------------------------------------------------------------------------------

Saturday, May 16, 2015

ESG On-Demand IDX Converter Demo + Video

Need to show:
 - cache is empty
 - cache is filled as data is requested
 - cache is filled with more data than is actually downloaded (due to coarse-to-fine streaming)
 - on-demand creation of idx metadata (by downloading visus.config from sample ESGF search site)
 - on-demand data conversion (from visus viewer, also would like to try from uvcdat)
 - what else???


0) Loading dataset, browsing 2d and 3d data, just using visus viewer. Speaking over this...

"The Earth Systems Grid On-Demand IDX data server provides streaming hierarchical versions of equivalent NetCDF climate data volumes in a user-directed manner such that specific timestep fields are converted just in time. This permits the bulk of data to remain on the server and facilitates interactive analysis and visualization by immediately sending results for specific data requests. Initial conversions are cached for future use, amortizing the cost of the conversion. This is especially important for large domain data because the hierarchical natures of IDX allows coarse resolution data to be streamed very quickly, providing a preview of the final data and facilitating interactivity. 

1) Overview (use overview image to explain system)

"Here is a visual overview of the system. It begins at the ESGF search page showin in the top-left. After locating the desired dataset, the user selects ViSUS IDX which will download the corresponding url configuration to load the selected volumes. 

1a) show esgf search page

"The link to produce the idx volumes from their corresponding cdat xml or netcdf file can be placed here on the esgf search results page. 

1b) Show secondary search page.

Since we can't modify the esgf pages directly, we have created a secondary page corresponding to the results of searching for a dataset on the test server pcmdi11."

1c) select dataset link

"When the user selects a dataset its corresponding IDX metadata is created and registered with an associated ViSUS server. This configuration will contain references to multiple IDX volumes, one for each domain of the climate dataset. 

1c) open .xml file.

An xml config file suitable for use with ViSUS Viewer is returned. It contains urls that can be loaded by any idx-compatible application.

1d) return to the esg overview image

"Once the user has the URL the dataset is loaded by the client application, such as the ViSUS Viewer or UV-CDAT as shown in the top-right. 

When data is requested, a remote query is made to the ViSUS server, which checks to see if the data already exists in the cache. If so, it sends the cached data immediately. Otherwise, it calls the climate data converter service which converts the data to IDX before sending it.

"We'll now demonstrate on-demand data conversion using the xml configuration we just downloaded.

4) Show the cache size page to indicate that no data has yet been converted.

"Notice that the server cache is currently empty."

5) Open the application from the command line.

"Let's start the ViSUS Viewer using the newly downloaded config.

6) Once the app is opened, navigate to the special variable. 

"The name of the dataset is acme-test, a high resolution simulation from ORNL. (stall) We'll look at a variable named TREFMXAV, the average daily maximum of 2-m temperature. (stall) Let's add a suitable palette."

7) show the cache size page

"Notice that some data has now now been converted."

8) Switch back and converte a couple more timesteps.

"We're able to retrieve new data on demand for any arbitrary timestep without loading the intervening timesteps. Notice how quickly data arrives due to the low conversion overhead."

9) Return to cache page and show it has increased in size.

"...and the cache continues to grow."

10) return to visusviewer and experiment some more. (Fade out video while speaking conclusion section summarizing work.)

"The Earth Systems Grid On-Demand IDX Data Server enables climate data to be provided on-demand at any resolution. Data is cached so that future requests do not incur the initial conversion overhead. Additional features such as dynamic data blending facilitate multi-model comparison, and the idx viewer can be built into external application such as UV-CDAT.

Thank you for watching.

-------------------------------------------------------------------------------

DOCS


python /scratch/cam/esg_server/scripts/cdat_converter_service.py --visusserver http://atlanta.sci.utah.edu/mod_visus --idxpath /usr/sci/cedmav/data/climate/ondemand/idx --xmlpath /usr/sci/cedmav/data/climate/ondemand/xml

(there is already plenty in the scripts directory, including ability to startup and shutdown using a file to keep track of pid of running server)

-------------------------------------------------------------------------------

Friday, October 23, 2015

NASA data retrieval attempt number 79

Notes: data should probably be converted compressed. Takes about 30% longer, but dramatically reduces volume sizes (1/16th in the case of NASA 3d climate data).

-------------------------------------------------------------------------------

How to copy a file from feedback to my laptop using ssh tunnels:

ssh -L 1234:feedback.llnl.gov:22 cam@atlanta.sci.utah.edu cat -
scp -P 1234 christensen41@127.0.0.1:/scratch/idx/raw/g5nr.nccs.nasa.gov/data/DATA/0.0625_deg/inst/inst30mn_3d_CO2_Nv/Y2007/M01/D01/c1440_NR.inst30mn_3d_CO2_Nv.20070101_0100z.nc4 .
-------------------------------------------------------------------------------

Some scripts to run 

for f in `cat redo_lat.txt`; do echo "*** $f"; if [ ! -f ../xml/${f%.list}.xml ]; then cdscan --exclude ps,eta,depth,lat,type -x /scratch/for_ganzberger1/idx/ondemand/xml/${f%.list}.xml -f $f > ${f%.list}.out 2>&1; fi done

for f in `cat published.txt`; do list=`echo $f | sed 's-/-_-g'`.list; find $f -name "*.nc*" > $list; done
for f in `ls _*.list`; do echo "*** $f"; g=${f#_}; xml=${g%.list}.xml; if [ ! -f ../xml/${xml} ]; then cdscan -x /scratch/for_ganzberger1/idx/ondemand/xml/${xml} -f $f > ${f%.list}.out 2>&1; fi done

-------------------------------------------------------------------------------

Tuesday, December 08, 2015
ESGF F2F Day 1

Guy from COG
http://earthsystemcog.org (look for CMPI6 whitepapers)

Dave Bader, Accelerated Climate Modeling for Energy (ACME) udpate
http://climatemodeling.science.energy.gov/projects/accelerated-climate-modeling-energy


http://geonetwork.nci.org.au - National Environmental Research Data Collections

birdhouse (data hosting service)

-------------------------------------------------------------------------------

Thursday, December 10, 2015

Common themes:

 - on-demand data conversion (e.g. for subsetting)
 - server-side computation (theme is "download less")
 - flexibility (e.g. a major challenge for WPS)
 - universality (e.g. common data format, conventions to compare models)

Affects existing infrastructure?
 - this is an optional service that anyone can use with almost no installation
 - even stubborn holdouts for data purity can secretly use it, just to see what the data you're downloading will look like ;)
 - the on-demand server code is open source (unless Dean says otherwise), and only needs access to data store, cache, and an http port
   -> any esgf node can install this

Next steps:

 - better handle temporal sampling
 - integrate with existing libraries to read data (make idx a first class citizen, in the same spirit of netcdf4, we need to move to the future
)
 

https://pcmdi9.llnl.gov/esgf-idp/openid/scicameron
@notherPassw0rd


-------------------------------------------------------------------------------
Thursday, December 10, 2015

Creating IDX of bluemarble

visusconvert --create bluemarble.idx --box "0 86399 0 43199" --time 0 12 time%02d/ --fields "data uint8[3]"
visusconvert --import world.topo.bathy.200401.3x21600x21600.A1.png --export bluemarble.idx --box "0 21599 21600 43199" --time 0 --field data
visusconvert --import world.topo.bathy.200401.3x21600x21600.A2.png --export bluemarble.idx --box "0 21599 0 21599" --time 0 --field data
visusconvert --import world.topo.bathy.200401.3x21600x21600.B1.png --export bluemarble.idx --box "21600 43199 21600 43199" --time 0 --field data
visusconvert --import world.topo.bathy.200401.3x21600x21600.B2.png --export bluemarble.idx --box "21600 43199 0 21599" --time 0 --field data
visusconvert --import world.topo.bathy.200401.3x21600x21600.C1.png --export bluemarble.idx --box "43200 64799 21600 43199" --time 0 --field data
visusconvert --import world.topo.bathy.200401.3x21600x21600.C2.png --export bluemarble.idx --box "43200 64799 0 21599" --time 0 --field data
visusconvert --import world.topo.bathy.200401.3x21600x21600.D1.png --export bluemarble.idx --box "64800 86399 21600 43199" --time 0 --field data
visusconvert --import world.topo.bathy.200401.3x21600x21600.D2.png --export bluemarble.idx --box "64800 86399 0 21599" --time 0 --field data

visusconvert --create bluemarble-compressed.idx --box "0 86399 0 43199" --time 0 12 time%02d/ --fields "data uint8[3] compressed"
visusconvert --import world.topo.bathy.200401.3x21600x21600.A1.png --export bluemarble-compressed.idx --box "0 21599 21600 43199" --time 0 --field data
visusconvert --import world.topo.bathy.200401.3x21600x21600.A2.png --export bluemarble-compressed.idx --box "0 21599 0 21599" --time 0 --field data
visusconvert --import world.topo.bathy.200401.3x21600x21600.B1.png --export bluemarble-compressed.idx --box "21600 43199 21600 43199" --time 0 --field data
visusconvert --import world.topo.bathy.200401.3x21600x21600.B2.png --export bluemarble-compressed.idx --box "21600 43199 0 21599" --time 0 --field data
visusconvert --import world.topo.bathy.200401.3x21600x21600.C1.png --export bluemarble-compressed.idx --box "43200 64799 21600 43199" --time 0 --field data
visusconvert --import world.topo.bathy.200401.3x21600x21600.C2.png --export bluemarble-compressed.idx --box "43200 64799 0 21599" --time 0 --field data
visusconvert --import world.topo.bathy.200401.3x21600x21600.D1.png --export bluemarble-compressed.idx --box "64800 86399 21600 43199" --time 0 --field data
visusconvert --import world.topo.bathy.200401.3x21600x21600.D2.png --export bluemarble-compressed.idx --box "64800 86399 0 21599" --time 0 --field data

-------------------------------------------------------------------------------
Friday, December 11, 2015

Thursday, December 17, 2015

building netcdf on my laptop for pidx

CPPFLAGS="-I/usr/local/pnetcdf/include -I/usr/local/hdf5/include -I/usr/local/mpi/include" LDFLAGS="-L/usr/local/hdf5/lib -L/usr/local/pnetcdf/lib -L/usr/local/mpi/lib -lmpi" ./configure --prefix=/usr/local/netcdf --enable-netcdf4 --enable-pnetcdf --enable-parallel-tests

-------------------------------------------------------------------------------

Friday, December 18, 2015

Converting nasa 3d .nc4 data using new and improved PIDX template-converter.c

3d dims: (5760,2881,72)
mpirun -np 8 ./tutorial/template_converter.app/Contents/MacOS/template_converter -l 720x2881x72 -i /Users/cam/data/uvcdat/infile.txt -v /Users/cam/data/uvcdat/invars.txt -f /Users/cam/data/uvcdat/c1440_NR.inst30mn_3d_CLOUD_Nv.20060616_1100z.idx

./tutorial/template_converter.app/Contents/MacOS/template_converter -l 5760x2881x72 -i /Users/cam/data/uvcdat/infile.txt -v /Users/cam/data/uvcdat/invars.txt -f /Users/cam/data/uvcdat/c1440_NR.inst30mn_3d_CLOUD_Nv.20060616_1100z.idx
